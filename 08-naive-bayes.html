<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <title>Naïve Bayes - Rui Qiu</title>
    <link rel="stylesheet" href="css/style.css">
</head>

<body class="sidebar">
    <nav>
        ◀️ <a href="index.html" class="return-button">Back</a>
        <ul>
            <li><a href="#r">R</a></li>
            <ul>
                <li><a href="#r-data">Data and scripts</a></li>
                <li><a href="#r-tuning">Model tuning</a></li>
                <li><a href="#r-feature">Feature importance</a></li>
                <li><a href="#r-viz">Visualization</a></li>
                <li><a href="#r-interpretation">Interpretation</a></li>
            </ul>

            <li><a href="#python">Python</a></li>
            <ul>
                <li><a href="#python-data">Data and scripts</a></li>
                <li><a href="#python-model">Model fitting</a></li>
                <li><a href="#python-feature">Feature importance</a></li>
                <li><a href="#python-interpretation">Interpretation</a></li>
            </ul>
        </ul>

        <svg xmlns=" http://www.w3.org/2000/svg">
            <path />
        </svg>

    </nav>
    <article id="top">
        <div class="subheading">Part 8: Naïve Bayes</div>
        <h1>A tale of two classifiers part 1</h1>

        <p><span>Rui Qiu</span></p>
        <p><span>Updated on 2021-11-21.</span></p>
        <br>

        <p><span>The main focus of this part of the portfolio and the next, will be utilizing two classifiers, namely
                naïve Bayes and SVM, to make the following two predictions:</span></p>
        <ol>
            <li><span>To predict the shot attempt results of Toronto Raptors (2020-2021 season).</span></li>
            <li><span>To predict the popularity of reddit threads based on its title.</span></li>
        </ol>

        <section id="r">
            <h2><span>Mixed record data with R</span></h2>
        </section>


        <section id="r-data">
            <h3><span>Data and scripts</span></h3>

            <blockquote>
                <ul>
                    <li><span>Data: </span><a
                            href='https://github.com/rexarski/box2box/blob/main/data/nba-pbp/2020-cleaned-by-team/team-shots-TOR.csv'><span>team-shots-TOR.csv</span></a>
                    </li>
                </ul>
                <ul>
                    <li><span>Script: </span><a
                            href='https://github.com/rexarski/box2box/blob/main/nba-nb-svm/nb-svm-script.R'><span>nb-svm-script.R</span></a>
                    </li>
                </ul>
            </blockquote>

            <p><span>The data consists of Toronto Raptors’ shot attempt data (mostly from 2020-2021 season) with the
                    following
                    general structure (with both numeric and categorical features):</span></p>

            <figure>
                <img src="img/08-tor-data.png" alt="tor-data" class="article-img-no-shadow">
                <figcaption>A glimpse of shot attempts data.</figcaption>
            </figure>

            <p><span>The data set is also split into a 70-30 train-test split, stratified by the result
                </span><code>made</code><span>.</span></p>

            <figure>
                <img src="img/08-tor-split.png" alt="tor-split" class="article-img-no-shadow">
                <figcaption>Splitting data.</figcaption>
            </figure>
        </section>

        <section id="r-tuning">
            <h3><span>Model tuning</span></h3>
            <p><code>{caret}</code><span> package is used to conduct a 10-fold cross validation.</span></p>
            <p><span>One naïve version of naïve bayes classifiers, without any parameter tuning, is concluded as
                    below:</span>
            </p>

            <figure>
                <img src="img/08-nb-m1.png" alt="nb-m1" class="article-img-no-shadow">
                <figcaption>nb.m1</figcaption>
            </figure>

            <p><span>Take a look at its confusion matrix and mean accuracy:</span></p>

            <figure>
                <img src="img/08-nb-m1-cm.png" alt="nb-m1-cm" class="article-img-no-shadow">
                <figcaption>Confusion matrix of nb.m1.</figcaption>
            </figure>

            <p><span>The overall accuracy is 81.12%, not bad. But apparently, the beauty of model selection is to fine
                    tune
                    some hyperparameters so that a model could improve itself.</span></p>
            <p><span>The following three hyperparameters are involved:</span></p>
            <ul>
                <li><code>usekernel</code><span>: use a kernel density estimate for continuous variables vs a Gaussian
                        density
                        estimate.</span></li>
                <li><code>adjust</code><span>: adjust the bandwidth of the kernel density (larger numbers mean more
                        flexible
                        density estimate)</span></li>
                <li><code>fL</code><span>: Laplace smoother</span></li>
            </ul>
            <p><span>Again, the grid search approach is used to accomplish this mission. In addition, the numeric
                    predictors
                    are preprocessed with </span><code>“center&quot;</code><span> and </span><code>“scale”</code><span>,
                    i.e.,
                    standardization.</span></p>
            <p><span>The resulted models are ordered by accuracy as the selection criterion. Top 5 of them are displayed
                    below:</span></p>

            <figure>
                <img src="img/08-nb-m2-top5.png" alt="nb-m2-top5" class="article-img-no-shadow">
                <figcaption>Top 5 models from nb.m2 by accuracy.</figcaption>
            </figure>

            <p><span>As the tuning process, it can be visualized as the following two charts as well:</span></p>

            <figure>
                <img src="img/08-nb-tuning.png" alt="nb-tuning" class="article-img">
                <figcaption>The tuning process of nb.m2.</figcaption>
            </figure>

            <p><span>The overall accuracy of tuned version naïve Bayes is 81.21%, slightly better than the untuned
                    version.</span></p>

            <figure>
                <img src="img/08-nb-m2-cm.png" alt="nb-m2-cm" class="article-img-no-shadow">
                <figcaption>Confusion matrix of nb.m2 (best model.)</figcaption>
            </figure>

            <p><span>Then the testing data is validated on selected naïve Bayes model, which returns the following
                    confusion
                    matrix:</span></p>

            <figure>
                <img src="img/08-nb-m2-pred-cm.png" alt="nb-m2-pred-cm" class="article-img-no-shadow">
                <figcaption>Confusion matrix of nb.m2 (best model) on testing data.</figcaption>
            </figure>

            <p><span>The result is very uplifting as the model actually outperforms itself with training data, which
                    eventually reaches an accuracy of 82.63%.</span></p>

        </section>

        <section id="r-feature">

            <h3><span>Feature importance</span></h3>

            <p><span>The <b>feature importance</b> in an object of
                    <code>train</code> is rather hard to extract when the naïve Bayes model is filled with imbalanced
                    categorical data. Given the fact that some features include more than a dozen of possible
                    categories, for example, the <code>player</code> has almost every active player on Toronto Raptors
                    roaster from last season, therefore, an alternative approach to represent how relatively important
                    those features are is to display all the conditional probabilities. The features which vary a lot
                    between two possible target variables are the ones of greater importance.
                </span></p>

            <figure>
                <img src="img/08-nb-feat.png" alt="nb-feat" class="article-img-no-shadow">
                <figcaption>Conditional probabilities of all features of nb.m2 (best model) on testing data. <br> Open
                    in a new tab for more details.</figcaption>
            </figure>
        </section>

        <section id="r-viz">
            <h3><span>Visualization of NB prediction</span></h3>
            <p><span>To see how the model works in practice, two visualizations are plotted to see the actual shots
                    made/missed and the predicted shots made/missed.</span></p>

            <figure>
                <img src="img/08-nb-actual-shot-attempts.png" alt="nb-actual" class="article-img">
                <figcaption>Actual shot attempts.</figcaption>
            </figure>

            <figure>
                <img src="img/08-nb-predicted-shot-attempts.png" alt="nb-predicted" class="article-img">
                <figcaption>Predicted shot attempts.</figcaption>
            </figure>

            <p><span>It turns out most of the deviations between the reality and the prediction are in midranges and
                    45-degree
                    three-pointers.</span></p>
            <p><span>Finally, a visualization is carried out to show the overall prediction accuracy of a fine-tuned
                    naïve
                    Bayes classifier:</span></p>

            <figure>
                <img src="img/08-nb-prediction-accuracy.png" alt="nb-prediction-accuracy" class="article-img">
                <figcaption>nb.m2 prediction accuracy.</figcaption>
            </figure>

        </section>

        <section id="r-interpretation">
            <h3><span>Interpretation</span></h3>
            <p><span>Recall previous attempts to predict if a shot is made with tree-based classifiers, the overall
                    accuracy
                    is barely over 50%. As mentioned last time, if a classifier, especially a binary classifier, only
                    performs like
                    flipping a coin, then it is really bad model.</span></p>
            <p><span>Is it due to the nature of the data? The answer is, “it’s possible”from last time.</span></p>
            <p><span>However, a very similar subset of data is used to capture the shot attempt pattern with a naïve
                    Bayes
                    classifier. The core idea is the following multinomial formula by Bayes theorem:</span></p>
            <p>$p(\mathbf{x}|C_k)=\frac{(\sum^n_{i=1}x_i)!}{\prod^n_{i=1}x_i!}\prod^n_{i=1}p_{ki}^{x_i}$</p>

            <p><span>where x’s are possible values of all the predictors.</span></p>
            <p><span>Then the class C is selected by the </span><code>argmax</code><span> of such a product. It’s more
                    or less
                    like a voting procedure, the class results in a higher probability will be the predicted class of
                    this
                    record.</span></p>
            <p><span>Intuitively, given a set of predictors, the classifier calculates the separate probabilities that
                    could
                    happen under a class </span><code>C_k</code><span>. Then the posterior probability is the joint
                    probability of
                    these separate independent probabilities.</span></p>
            <p><span>However, one should note that in reality, the assumption of independence among features/variables
                    is very
                    rare.</span></p>
            <p><span>Still, the naïve Bayes classifier gives a decent prediction to start this “tale of two
                    classifiers.” More
                    to be discussed in the next part of this portfolio.</span></p>
        </section>

        <section id="python">
            <h2><span>Text data with Python</span></h2>
        </section>

        <section id="python-data">
            <h3><span>Data and scripts</span></h3>

            <blockquote>
                <ul>
                    <li>
                        <p><span>Data</span></p>
                        <ul>
                            <li><a
                                    href='https://github.com/rexarski/box2box/blob/main/data/nba-reddit/nba-top-100-threads-this-year-cleaned.csv'><span>Raw
                                        data</span></a></li>
                            <li><a
                                    href='https://github.com/rexarski/box2box/blob/main/nba-nb-svm/reddit-unigram-tfidf.csv'><span>Cleaned
                                        data</span></a></li>
                        </ul>
                    </li>
                </ul>
                <ul>
                    <li><span>Script: </span><a
                            href='https://github.com/rexarski/box2box/blob/main/nba-nb-svm/reddit-nb-svm.ipynb'><span>reddit-nb-svm.ipynb</span></a>
                    </li>
                </ul>
            </blockquote>

            <p><span>Just like the record data, the text data in this section is also from the tree-based classification
                    chapter.</span></p>
            <p><span>The data is preprocessed by removing stopwords, conversion to lower case, and then calculated as
                    Tf-idf.
                    What is different from last time is that the texts are not stripped as bigrams but unigrams this
                    time.</span>
            </p>
            <p><span>Additionally, the target variable </span><code>upvote_ratio</code><span> is replaced by
                </span><code>popularity</code><span>, a categorical variable indicating how popular the thread is within
                    the
                    time span of a year. To make the classes more balanced, the variable
                </span><code>popularity</code><span> is
                    trivially set as either </span><code>Extremely Popular</code><span> (top 50) or
                </span><code>Very Popular</code><span> (top 51-100).</span></p>
            <p><span>The tf-idf values are also standardized, although the matrix is still very sparse due to the nature
                    of
                    text data that not too many words are repeated in each document.</span></p>
            <p><span>The general structure of the preprocessed data looks like this:</span></p>

            <figure>
                <img src="img/08-reddit-data.png" alt="reddit-data" class="article-img-no-shadow">
                <figcaption>Reddit threads text data.</figcaption>
            </figure>

            <p><span>A wordcloud is also plotted like last time.</span></p>

            <figure>
                <img src="img/08-reddit-title-wordcloud2.png" alt="reddit-title-wordcloud2" class="article-img">
                <figcaption>A wordcloud of text data.</figcaption>
            </figure>

            <p><span>Of course, the text data is also split into a pair of 80-20 training/testing sets.</span></p>
        </section>

        <section id="python-model">
            <h3><span>Model fitting</span></h3>

            <p><span>Then the preprocessed data is fitted in a multinomial naïve Bayes model. The resulted confusion
                    matrix is
                    plotted below:</span></p>

            <figure>
                <img src="img/08-reddit-nb-cm.png" alt="reddit-nb-cm" class="article-img">
                <figcaption>Confusion matrix of naïve Bayes classifier on text data.</figcaption>
            </figure>

            <figure>
                <img src="img/08-reddit-nb-metrics.png" alt="reddit-nb-metrics" class="article-img-no-shadow">
                <figcaption>Metrics table of naïve Bayes classifier on text data.</figcaption>
            </figure>

            <p><span>The overall accuracy is 60%.</span></p>
            <p><span>Furthermore, a naïve Bayes model in fact calculates the probabilties of each class. The following
                    is a
                    histogram displaying the probabilties of the test records to be both classes. Orange stands for
                </span><code>Extremely Popular</code><span>, while blue means
                </span><code>Very Popular</code><span>.</span></p>

            <figure>
                <img src="img/08-reddit-nb-prediction-hist.png" alt="reddit-nb-prediction-hist" class="article-img">
                <figcaption>Prediction histogram of naïve Bayes classifier on text data.</figcaption>
            </figure>
            <p><span>Finally, an ROC curve is plotted for the “goodness” of such a model, which will be discussed in the
                    later
                    paragraph.</span></p>

            <figure>
                <img src="img/08-reddit-nb-roc.png" alt="reddit-nb-roc" class="article-img">
                <figcaption>ROC of naïve Bayes classifier on text data.</figcaption>
            </figure>
        </section>

        <section id="python-feature">
            <h3><span>Feature importance</span></h3>

            <p><span>The <b>feature importance</b> of the naïve Bayes model on text data, can be represented in the
                    following numpy array. Since the DTM of text data used in this part is rather large and sparse, the
                    feature importance has a hard to interpret length of "relative importance" accordingly. A further
                    dive into the data will reveal that the one with the most importance is the word <b>"deep"</b>,
                    which could be related to some amazing long-distance three-pointer shootings in the game.</span></p>

            <p><span>Besides, the feature standard deviations are also included:</span></p>

            <figure>
                <img src="img/08-reddit-nb-feat.png" alt="reddit-nb-feat" class="article-img-no-shadow">
                <figcaption>Feature importance of naïve Bayes classifier on text data.</figcaption>
            </figure>


        </section>

        <section id="python-interpretation">
            <h3><span>Interpretation</span></h3>

            <p><span>Without doubt, the naïve Bayes classifier with an accuracy of 60% is not ideal, especially for a
                    testing
                    data set of size 20. It’s just slightly better than flipping a coin, or randomly guess everything is
                    of one
                    class only. One could not even be sure if the 60% is just being “lucky”. It is almost conclusive to
                    say that
                    predicting the popularity barely based on the word choice of titles is impossible.</span></p>
            <p><span>The ROC curve from the last plot also demonstrates the model's poor performance. In contrast, a
                    decent model should close in the top left corner as much as possible.</span></p>
            <p><span>Nevertheless, there are some highlights of such a model:</span></p>
            <ol>
                <li><span>The training process is extremely fast. (Thanks to the small size of the data as well.)</span>
                </li>
                <li><span>The results are easy to interpret. A vector of probabilities will directly tell the reader
                        what the
                        model “thinks” the record belongs to.</span></li>
                <li><span>The model almost does not need any tuning.</span></li>
            </ol>
            <p><span>But we also need to be careful about the assumption that each variable should be independent of
                    each
                    other. This rarely happens in real life, including this case. Just imagine whether the occurrences
                    of the
                    following two words are independent:“Washington” and “Wizards”.</span></p>
            <p><span>No models are perfect, some even fail its own assumptions.</span></p>
        </section>


        <hr>

        <footer class="footer">
            <p>Copyright &copy; 2021 <a href="https://qrui.xyz">Ruì Qiū</a></p>
        </footer>
    </article>

    <script src="js/script.js"></script>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
        };</script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
</body>

</html>